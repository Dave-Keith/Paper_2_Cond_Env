---
output:
  # bookdown::word_document2:
  #   fig_caption: yes
  #   number_sections: false
  # fontsize: 12pt
  # sansfont: Liberation Sans
  # mainfont: Liberation Sans
  # classoption: twocolumn
  # language: english
  # bookdown::html_document2: default
  bookdown::pdf_document2:
      keep_tex: yes
      number_sections: false
      toc: no
      # includes:
      # in_header: "styling.sty"
  language: english
  classoption: twocolumn
# End of options to set
title: "The Scallop who wished for a King: How utilzing an environmental relationship can improve prediction of condition yet result in less accurate science advice."
author: |
  David M. Keith^1^,
  Jessica A. Sameoto^1^,
  Xiaohan Liu^2^, 
  Emmanuel Devred^1^,
  Freya A. Keyser, and 
  Catherine Johnson^1^
address: |
  ^1^Bedford Institute of Oceanography\
     Fisheries and Oceans Canada, 1 Challenger Dr.\
     Dartmouth Nova Scotia, B2Y 4A2, Canada\
  ^2^UK\
month: November # fill in
year: 2020
report_number: nnn
region: Maritimes Region
author_list: "Keith, D.M., Sameoto, J.A., Keyser, F., Liu, X., Devred, E., Johnson, C."
abstract: |
  Oceanographic conditions are known to influence natural fluctuations in fish stocks, however disentangling environmental variability from fishing effects remains challenging despite significant efforts to improve science advice through an ecosystem approach to fisheries management (EAFM). For the worldâ€™s largest wild scallop fisheries, the sea scallop (*Placopecten magellanicus*) found off the northeastern United States and eastern Canada annual variations in growth underlie major fluctuations in catch rate and yield.  Inter-annual variability in the relative size of the harvested meat is measured using an allometric relationship between scallop meat-weight and shell height, known as scallop condition (SC), which is used to estimate the population biomass. When estimating the biomass for the current fishing year SC is an unknown parameter which is predicted based on a biological-only model. The purpose of this study was to investigate the development of predictive models of scallop growth that incorporated environmental covariates.  The environmental covarites used were Sea Surface Temperature (SST), Bottom Temperature (BT), Chlorophyll-a (CHL), and Mixed Layer Depth (MLD). The preferred predictive models incorporated either SST or BT and generally lead to more accurate predictions of SC than the current biologicaly only model. A retrospective analysis was then undertaken in which the current year biomass estimates from the stock assessment model were compared using the currently implemented bioloigical-only model and the best performing environmental models. The results of this retrospective analysis indicated that there is a small positive bias of approximately 2% in the model biomass predictions.  The biological-only model tends to predict that SC will be lower than the predictive environmental models, thus the use of the environmental models results in a slight, statistically insignificant, increase in the bias of the model biomass predictions.  The underestiamte of SC in the biological model helps to offset the tendency for the assessment model to overestimate biomass in the current fishing year. These results highlight the challenges of attempting to incorporate environmental information into existing stock assessment frameworks.  The development of next generation integrated stock assessment frameworks that incorporate environmental considerations directly into the model structure may provide a more flexible means of developing science advice that can operationlize EAFM considerations.
  
header: "Draft working paper --- Do not cite or circulate" # or "" to omit

knit: bookdown::render_book
link-citations: true
bibliography: Y:/Zotero/MAR_SABHU.bib
csl: Y:/Zotero/styles/canadian-journal-of-fisheries-and-aquatic-sciences.csl
# Any extra LaTeX code for the header:
#  Note that if you need to include more than one package you will have to have them on the same line like this:
header-includes: 
  - \usepackage{tikz} \usepackage{pdflscape} \usepackage{float}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
#  - \newcommand{\be}{\begin{equation}}
#  - \newcommand{\ee}{\end{equation}}
  - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}

---


```{r setup, echo=FALSE, cache=FALSE, message=FALSE, results='hide', warning=FALSE}
library(knitr)

##### Bring in the data and functions
library(MASS)
library(readxl)
library(bbmle)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(mgcv)
library(lubridate)
library(pander)
library(scales)
library(R.matlab)
library(cowplot)
require(formatR)
require(tidyverse)
require(dplyr)
require(tidyr)
library(ggthemes)

knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
# Set the Workding directory.
direct.proj <- "D:/Github/Paper_2_Cond_Env/"
n.boots =2 # Just temporary for testing, when you notice this later delete this.

#Functions
funs <- c("https://raw.githubusercontent.com/Mar-Scal/Assessment_fns/master/Maps/pectinid_projector_sf.R",
          "https://raw.githubusercontent.com/Mar-Scal/Assessment_fns/master/Maps/convert_inla_mesh_to_sf.R",
          "https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Maps/centre_of_gravity.R",
          "https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Maps/add_alpha_function.R",
          "https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Model/projections.r")
# Now run through a quick loop to load each one, just be sure that your working directory is read/write!
for(fun in funs) 
{
  download.file(fun,destfile = basename(fun))
  source(paste0(getwd(),"/",basename(fun)))
  file.remove(paste0(getwd(),"/",basename(fun)))
}
source(paste0(direct.proj, "Scripts/correlation_table_function_revised.r"))

# A couple custom functions I may or may not use
factor.2.number <- function(x) {as.numeric(levels(x))[x]} # My friend factor.2.number
# Function in case you need it for transforming propotion data to not have 0's and 1's.  
beta.transform <- function(dat,s=0.5)  (dat*(length(dat)-1) + s) / length(dat)
# Just so this code is easily portable over to our eventual Res Doc..

# The prediction prop function


if (is_latex_output()) {
  knitr_figs_dir <- "knitr-figs-pdf/"
  knitr_cache_dir <- "knitr-cache-pdf/"
  fig_out_type <- "png"
} else {
  knitr_figs_dir <- "knitr-figs-docx/"
  knitr_cache_dir <- "knitr-cache-docx/"
  fig_out_type <- "png"
}
fig_asp <- 0.618
fig_width <- 9
fig_out_width <- "6in"
fig_dpi <- 180
fig_align <- "center"
fig_pos <- "htb"
opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  fig.path = knitr_figs_dir,
  cache.path = knitr_cache_dir,
  fig.asp = fig_asp,
  fig.width = fig_width,
  out.width = fig_out_width,
  echo = FALSE,
  #  autodep = TRUE,
  #  cache = TRUE,
  cache.comments = FALSE,
  dev = fig_out_type,
  dpi = fig_dpi,
  fig.align = fig_align,
  fig.pos = fig_pos
)
options(xtable.comment = FALSE)
options(kableExtra.latex.load_packages = FALSE)


# Don't use scientific notation please!!
options(scipen=999)

```

```{r data-load, echo=FALSE, cache=T, message=FALSE, results='hide', warning=FALSE}

#####  Data Load....
sh.dat<- readRDS("Data/sh_dat.RDS")
vonB <- readRDS("Data/vonb_dat.RDS")
gb.proj.catch <- readRDS("Data/gb_proj_catch.RDS")
gb.fish.dat <- readRDS("Data/gb_fish_dat.RDS")
DD.out <- readRDS("Data/gb_model_output.RDS")
base.cond.ts <- read.csv(paste0(direct.proj, "Data/Full_may_aug_condition_ts.csv"))
# Here we load in the raw data and get everything ready for further analyses.
#----------------------------------------------------------------------------------
# RAW Data Section
# Bring in the raw data.
sst.raw.in <- read.csv(paste0(direct.proj,"Data/dfo_sst_ts.csv"))

# Bring in the GB and GBa polygons


##### Done with data loading... Set some variables for the rest of the show..

### Now some basic data prep for the next stage of fun
sst.raw.dat <- sst.raw.in %>% dplyr::select(year,month,GBS_weighted_median)
names(sst.raw.dat) <- c("Year","Month","covar")

# dat.aug <- read_xlsx("Data/Scallop_condition_with_same_SST_covariates.xlsx",sheet = "Aug",col_types = "numeric")
# dat.may <- read_xlsx("Data/Scallop_condition_with_same_SST_covariates.xlsx",sheet = "May",col_types = "numeric")

# We want a month-Year combo field for these
sst.raw.dat$date <- dmy(paste(01,sst.raw.dat$Month,sst.raw.dat$Year, sep="/"))
sst.raw.dat$Year <- year(sst.raw.dat$date)

# Building these by hand because I don't know where the SST data in the XLSX files came from
dat.may <- data.frame(Year=base.cond.ts$year[base.cond.ts$year>1997], 
                      Condition=base.cond.ts$may[base.cond.ts$year>1997])


# Now we want these as anomolies from average, so starting with SST...
sst.raw.dat$anom <- sst.raw.dat$covar - mean(sst.raw.dat$covar,na.rm=T)

# Now we want to remove the "seasonal" signal, one way to do this would be to remove the 
# mean temperature anomoly found for each month, so for January, subtract off the mean Jan temp anomoly...
for(i in 1:12) 
{  
  # Sea surface temp
  sst.raw.dat$anom.seasonal[sst.raw.dat$Month == i] <- 
    sst.raw.dat$anom[sst.raw.dat$Month == i] -  mean(sst.raw.dat$anom[sst.raw.dat$Month == i],na.rm=T)
}

#Now we can get the monthly SST, CHla, and MLD estimates, this is used for the first figure in the paper.
# chl.mon <- aggregate(covar~ Month,data = chl.raw.dat,FUN=mean)
# chl.mon$sd <- aggregate(covar~ Month,data = chl.raw.dat,FUN=sd)[,2]
# chl.mon$variable <- "chl"
month.dat <- aggregate(covar~ Month,data = sst.raw.dat,FUN=mean)
month.dat$sd <- aggregate(covar~ Month,data = sst.raw.dat,FUN=sd)[,2]
month.dat$variable <- "sst"



# Here we get the data we want for the correlation figures, 
dat.list <- list(sst = sst.raw.dat)
cond.list <- list(may = dat.may)
res <- NULL
for(i in 1:length(dat.list))
{
  for(j in 1:length(cond.list))
  {
    res[[paste(names(dat.list)[i],names(cond.list)[j],sep="_")]] <- 
      cor_dat(dat = dat.list[[i]], response = cond.list[[j]])
  }  # end for(j in 1:length(cond.list))
} # end for(i in 1:length(dat.list))



```


