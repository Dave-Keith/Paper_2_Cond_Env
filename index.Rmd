---
output:
  # bookdown::word_document2:
  #   fig_caption: yes
  #   number_sections: false
  # fontsize: 12pt
  # sansfont: Liberation Sans
  # mainfont: Liberation Sans
  # classoption: twocolumn
  # language: english
  # bookdown::html_document2: default
  bookdown::pdf_document2:
      keep_tex: yes
      number_sections: false
      toc: no
      # includes:
      # in_header: "styling.sty"
  language: english
  classoption: twocolumn
# End of options to set
title: "An environmental paradox: How a strong environmental relationship increased the uncertainty of science advice."
author: |
  David M. Keith^1^,
  Jessica A. Sameoto^1^,
  Xiaohan Liu^2^,\
  Emmanuel Devred^1^,
  Freya Keyser, and 
  Catherine Johnson^1^
address: |
  ^1^Bedford Institute of Oceanography\
     Fisheries and Oceans Canada, 1 Challenger Dr.\
     Dartmouth Nova Scotia, B2Y 4A2, Canada\
  ^2^UK\
abstract: |
  Oceanographic conditions are known to influence natural fluctuations in fish productivity. Despite significant efforts to improve tactical science advice through an ecosystem approach to fisheries management (EAFM), disentangling environmental variability from fishing effects remains challenging . For the worldâ€™s largest wild scallop fishery, the sea scallop (*Placopecten magellanicus*) fishery off the northeastern United States and eastern Canada, inter-annual variations in growth underlie major fluctuations in catch rate and yield.  The relative size of the harvested meat is measured using scallop condition (SC), which is predicted using an allometric relationship between scallop meat-weight and shell height. In the sea scallop stock assessment model, the SC prediction for a given year is used to convert population abundance estimates to population biomass. The purpose of this study was to evaluate the effect of incorporating Sea Surface Temperature (SST) into predictive models of SC on tactical science advice. The preferred predictive models generally lead to more accurate predictions of SC than the current biology-only model. A retrospective analysis was then undertaken to compare the annual biomass estimates from the stock assessment model using the biology-only model to the corresponding estimates from the best performing environmental models. The results of this retrospective analysis indicated that there was a small positive bias of approximately 2% in the model biomass predictions using each of the predictive SC models. The biology-only model tended to predict a lower SC than the predictive environmental models, thus the use of the environmental models resulted in a small (statistically insignificant) increase in the bias of the model biomass predictions. These results highlight the challenges of attempting to incorporate environmental relationships into tactical science advice using traditional stock assessment frameworks. The relevance of the environmental variables, potential for impact, and the environmental data availability should be considered when attempting to develop EAFM tools. The development of next-generation integrated stock assessment frameworks that incorporate environmental considerations directly into the model structure may provide a more flexible and robust means of developing tactical science advice that can operationalize EAFM considerations.
  
knit: bookdown::render_book
link-citations: true
bibliography: Y:/Zotero/MAR_SABHU.bib
csl: Y:/Zotero/styles/canadian-journal-of-fisheries-and-aquatic-sciences.csl
# Any extra LaTeX code for the header:
#  Note that if you need to include more than one package you will have to have them on the same line like this:
header-includes: 
  - \usepackage{tikz} \usepackage{pdflscape} \usepackage{float}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
#  - \newcommand{\be}{\begin{equation}}
#  - \newcommand{\ee}{\end{equation}}
  - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}

---


```{r setup, echo=FALSE, cache=FALSE, message=FALSE, results='hide', warning=FALSE}
library(knitr)

##### Bring in the data and functions
library(MASS)
library(readxl)
library(bbmle)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(mgcv)
library(lubridate)
library(pander)
library(scales)
library(R.matlab)
library(cowplot)
require(formatR)
require(tidyverse)
require(dplyr)
require(tidyr)
library(ggthemes)
library(sf)
library(grid)

knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
# Set the Workding directory.
direct.proj <- "D:/Github/Paper_2_Cond_Env/"
n.boots =2 # Just temporary for testing, when you notice this later delete this.

#Functions
funs <- c("https://raw.githubusercontent.com/Mar-Scal/Assessment_fns/master/Maps/pectinid_projector_sf.R",
          "https://raw.githubusercontent.com/Mar-Scal/Assessment_fns/master/Maps/convert_inla_mesh_to_sf.R",
          "https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Maps/centre_of_gravity.R",
          "https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Maps/add_alpha_function.R",
          "https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Model/projections.r")
# Now run through a quick loop to load each one, just be sure that your working directory is read/write!
for(fun in funs) 
{
  download.file(fun,destfile = basename(fun))
  source(paste0(getwd(),"/",basename(fun)))
  file.remove(paste0(getwd(),"/",basename(fun)))
}

# A couple custom functions I may or may not use
factor.2.number <- function(x) {as.numeric(levels(x))[x]} # My friend factor.2.number
# Function in case you need it for transforming propotion data to not have 0's and 1's.  
beta.transform <- function(dat,s=0.5)  (dat*(length(dat)-1) + s) / length(dat)
# Just so this code is easily portable over to our eventual Res Doc..

# The prediction prop function


if (is_latex_output()) {
  knitr_figs_dir <- "knitr-figs-pdf/"
  knitr_cache_dir <- "knitr-cache-pdf/"
  fig_out_type <- "png"
} else {
  knitr_figs_dir <- "knitr-figs-docx/"
  knitr_cache_dir <- "knitr-cache-docx/"
  fig_out_type <- "png"
}
fig_asp <- 0.618
fig_width <- 9
fig_out_width <- "6in"
fig_dpi <- 180
fig_align <- "center"
fig_pos <- "htb"
opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  fig.path = knitr_figs_dir,
  cache.path = knitr_cache_dir,
  fig.asp = fig_asp,
  fig.width = fig_width,
  out.width = fig_out_width,
  echo = FALSE,
  #  autodep = TRUE,
  #  cache = TRUE,
  cache.comments = FALSE,
  dev = fig_out_type,
  dpi = fig_dpi,
  fig.align = fig_align,
  fig.pos = fig_pos
)
options(xtable.comment = FALSE)
options(kableExtra.latex.load_packages = FALSE)


# Don't use scientific notation please!!
options(scipen=999)

```

```{r data-load, echo=FALSE, cache=T, message=FALSE, results='hide', warning=FALSE}

#####  Data Load....
sh.dat<- readRDS("Data/sh_dat.RDS")
vonB <- readRDS("Data/vonb_dat.RDS")
gb.proj.catch <- readRDS("Data/gb_proj_catch.RDS")
gb.fish.dat <- readRDS("Data/gb_fish_dat.RDS")
DD.out <- readRDS("Data/gb_model_output.RDS")
base.cond.ts <- read.csv(paste0(direct.proj, "Data/Full_may_aug_condition_ts.csv"))
# Here we load in the raw data and get everything ready for further analyses.
#----------------------------------------------------------------------------------
# RAW Data Section
# Bring in the raw data.
sst.raw.in <- read.csv(paste0(direct.proj,"Data/dfo_sst_ts.csv"))

# Bring in the GB and GBa polygons

# Now grab the coordinates for the survey boundaries for GBa and gbb, inside of the survey_boundaries zip.
temp <- tempfile()
# Download this to there
download.file("https://raw.githubusercontent.com/Mar-scal/GIS_layers/master/survey_boundaries/survey_boundaries.zip", temp)
# Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# And we get GBa and GBb from there
gba.shape <- st_read(dsn = paste0(temp2,"/GBa.shp"))

# Now grab the coordinates for the survey boundaries for GBa and gbb, inside of the survey_boundaries zip.
temp <- tempfile()
# Download this to there
download.file("https://raw.githubusercontent.com/Dave-Keith/Paper_2_Cond_Env/master/Data/boundary/boundary.zip", temp)
# Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# And we get GBa and GBb from there
gb.iso <- st_read(dsn = paste0(temp2,"/GB_PolyForRemoteSensing.shp"))

# Trim the gba.shape to be withing the gb.iso and get rid of that silliness with eez...
gba.shape <- st_intersection(gba.shape,gb.iso)

##### Done with data loading... Set some variables for the rest of the show..

### Now some basic data prep for the next stage of fun
sst.raw.dat <- sst.raw.in %>% dplyr::select(year,month,GBS_weighted_median)
names(sst.raw.dat) <- c("Year","Month","covar")

# dat.aug <- read_xlsx("Data/Scallop_condition_with_same_SST_covariates.xlsx",sheet = "Aug",col_types = "numeric")
# dat.may <- read_xlsx("Data/Scallop_condition_with_same_SST_covariates.xlsx",sheet = "May",col_types = "numeric")

# We want a month-Year combo field for these
sst.raw.dat$date <- dmy(paste(01,sst.raw.dat$Month,sst.raw.dat$Year, sep="/"))
sst.raw.dat$Year <- year(sst.raw.dat$date)

# Building these by hand because I don't know where the SST data in the XLSX files came from
dat.may <- data.frame(Year=base.cond.ts$year[base.cond.ts$year>1997], 
                      Condition=base.cond.ts$may[base.cond.ts$year>1997])


# Now we want these as anomolies from average, so starting with SST...
sst.raw.dat$anom <- sst.raw.dat$covar - mean(sst.raw.dat$covar,na.rm=T)

# Now we want to remove the "seasonal" signal, one way to do this would be to remove the 
# mean temperature anomoly found for each month, so for January, subtract off the mean Jan temp anomoly...
for(i in 1:12) 
{  
  # Sea surface temp
  sst.raw.dat$anom.seasonal[sst.raw.dat$Month == i] <- 
    sst.raw.dat$anom[sst.raw.dat$Month == i] -  mean(sst.raw.dat$anom[sst.raw.dat$Month == i],na.rm=T)
}

#Now we can get the monthly SST, CHla, and MLD estimates, this is used for the first figure in the paper.
# chl.mon <- aggregate(covar~ Month,data = chl.raw.dat,FUN=mean)
# chl.mon$sd <- aggregate(covar~ Month,data = chl.raw.dat,FUN=sd)[,2]
# chl.mon$variable <- "chl"
month.dat <- aggregate(covar~ Month,data = sst.raw.dat,FUN=mean)
month.dat$sd <- aggregate(covar~ Month,data = sst.raw.dat,FUN=sd)[,2]
month.dat$variable <- "sst"

```


